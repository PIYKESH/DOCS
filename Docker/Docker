Docker 
1- Docker is a advanced version of virtualization in form of Containerization.
2- Hypervisor is used for virtualization but in case of Containerization we use Docker Engine.
3- Docker provides all the dependencies in an environment which prevents conflicts for code version or it can be anything. 
4- Docker uses Container on the host O.S to run the applications to use the same Linux kernel as system on the host computer, 
   rather than creating new a whole new virtual O.S.
5- We can install in any O.S but docker engine runs natively on Linux Distribution. 
6- Docker was first released in march 2013 by Soloman Hykes and Sebastian Pahl, Which is written in GO language.
7- Docker is a set of platform as a service that uses O.S level virtualization where VM ware uses Hardware level Virtualization.
8- Disadvantages:- a) Docker is not a good solution for application that requires rich GUI.
				   b) Docker does not provide cross-platform compatibility if an application is designed to run in docker.
				   c) Docker is suitable when Dev O.S and OPS O.S are same, If the O.S is different then we should use VM.


Microservices Architecture-A variant of the service-oriented architecture (SOA) structural style- arranges an application as a collection 
of loosely coupled services. In a microservices arch, , services are fine grained and the protocols are light weight.

Segregated functionality into smaller separate services each with a single responsibility, Scales out by deploying each service 
independently, Loosely coupled, Can be written by smaller teams, Each micro services can own it's own data/database.

Monolith Architecture- Built as a single unit, deployed as single unit, Duplicated on each server ex- 3 tier apps.

From Monolith to microservices
*Break your application/systems in small units.
*Use the strangler pattern.

Microservices- Benefits
*Improved fault isolation
*Eliminate vendor or technology lock-in
*Ease of understanding
*Smaller and faster deployments
*Scalability

Microservices- Drawbacks
*Complexity is added to resolve complexity issues
  -Is your team trained, ready and has made POC's
  -Don't start with a complex infrastructure.
*Testing may appear simpler but is it?
*Deployment may appear simpler but is it?
  Hard to do with multiple teams.
  One microservices updated can impact many microservices.
*Multi databases?
*Latency issues
*Transient Errors
*Multiple point of failures.

Container- A unit of software/deployment, code/runtime/system tools system libraries

Why containers?
-Move faster by deploying smaller units
-Use fewer resources
-Fit more into the same host
-Faster automation
-Portability

VM vs Containers

Virtual Machine 
*Large footprint
*Slow to boot
*Ideal for long running tasks

Container
*Lightweight
*Quick to start
*Portable
*Ideal for short lives tasks

Docker commands like-: docker --version
                       docker ps , docker ps -a, docker run -image name, docker stop -image name, docker rmi image name,
                       docker pull, docker push, 

Rest refer to images in the folder.

Docker file {syntax}

example: FROM ngnix:alpine
         COPY ./usr/share/ngnix/html

#build
docker build -t webserver-image:v1.

#run
docekr run -d -p 8080:80 webserver-image:v1
#dicplay
curl localhost:8080

example: FROM alpine --> It's the base image 
         RUN apk add -update nodejs nodejs-npm --> install Node and NPM using package manager
         COPY ./src --> copy the files from the build context
         WORKDIR /src 
         RUN npm install -->Run a command
         EXPOSE 8080:80 -->Add a metadata
         ENTRYPOINT ["node", "./app.js"] -->what to run

Docker tag=> Create a target image
             * name:tag
                   *myimage:v1
             *repository/name:tag
                   *myacr.azurecr.io/myimage:v1

Data Persisting

Containers are ephemerous and stateless
* You usually don't store data in containers
* Non-persistent data
  *Locally on a writable layer
  *when containers are destroyed, so the data inside them

* Persistent data
  *Stored outside the container in a volume.
  *A volume is mapped to a logic folder.

Volumes
* Maps a folder on the host to a logical folder in the container.

docker volumes command: docker volume create, docker create volume ls, docker volume inspect [volumeName], docker volume rm [volumeName],
docker volume prune

Mapping volume with container

Step-1 Crate volume
Step-2 Inspect the volume
Step-3 List the volume
Step-4 Run a container with volume
       docker run -d --name devtest -v myvol:/app ngnix:latest 

Better to mapping the local folder instead the vol
       docker run -d --name devtest -v d:/test:/app ngnix:latest
  
Inspect the container
       docker inspect devtest 

Docker yaml 

*YAML: Yaml Ain't Markup Language
*Human friendly data serialization standard
*Used by docker-compose and Kubernetes.



# comment in yaml looks like this

key: value
another_key: Another value goes here
a_number_value: 100

#Nesting uses indentation. 2 sapces indent is preferred (but not requirted)
a_nested_map:
  key: value
  another_key: Another Value
  another_nested_map:
    hello: world

# Sequence (quivalent to list or arrays) look like this
# (note that the '_' counts as indentation )

a_sequence:
  - Item: 1
  - Item: 2


Docker Compose Concepts
*Define and run multi-container's application
*Define using YAML files.
*Run using the docker CLI with the compose plugin
    *Docker compose
*Compose Specs    
    *https://compose-spec.io

Compose V2
*GA at DockerCon Live 2022
*Incorporates the docker compose command into the docker CLI
  *You type docker compose instead of docker compose
  *Drop-in replacement for docker compose
  *docker compose commands maps directly to the docker compose ones
*Installed with docker desktop
  *Linux: apt-get install docker compose-plugin
*Written in GO
  *docker compose is written in python

*In summary, it's simply the faster version of the good old docker compose tool that is shipped as plugin instead of a python app.



Docker Compose file Syntax containing three containers

version: '3.9'

services:
  webapi1:
    image: academy.azure.io/webapi1
    ports:
      - '8081:80'
    restart: always

  webapi2:
    image: academy.azure.io/webapi2
    ports:
      - '8082:80'
    restart: always
  
  apigatway:
    image: academy.azurecr.io/apigateway
    ports: 
      - '80:80'
    restart: always


Docker Compose Features:-

Resource Limits example:
services:
  redis:
    image: redis:alpine
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 150M
        reservations:
          cpus: '0.25'
          memory: 20M  ----->inital allocation

Environment Variables:-
*We can set key value pair for each service under the env. variables for an running instance.

Example:

services:
  web:
    image: ngnix:alpine
    environment:
      - DEBUG=1
      - FOO=BAR

*But these values can be overridden by docker compose up -d -e DEBUG=0
*We can reference an env variable using the docker curly bracket syntax this way we can set the variable on your machine or 
server and use it directly in the compose file you can compose values in a file that you will name .env located in the same folder 
as the compose file.

*To use env variables we have to create .env file in the same folder where we have to define variable which we will be using 
 and to call it in the yaml file we have to use ${variable_name}.
 
*Suppose we have sql server and we want to define env variable by two ways we can do that either by defining in the yaml file or
 by using the separate file for definig env, Ex- define my-sql as service and for credential we use file mysqlcongifg.env in that 
 we can store are password or config and later we can use that file in compose file by using env_file: - mysqlconfig.env 
 
 
Profiles:- Suppose we have 3 services in compose file and we want to use only two and other can be used only when enabled so
           we can use profile for that service and later we can enable by terminal to execute and run it. Ex- after defining
		   service then declare profiles: - rediscache
terminal command: docker compose --profile rediscache up -d
                  docker compose --profile rediscache down --volumes  	
		   
Networking:- To define custom network we define in the same yaml file and define by custom name. Also if we didn't use the network 
             name in the service it will go out to default and the one which we define it will use that common.
    
Example:-
services:
  web:
    image: ngnix:alpine
    ports:
      - '8080:80'
	networks:
	  - my_network

  db:
    image: postgress
    ports:
      - '5432'
	networks:
	  - my_network  
	  
netoworks:
  my_network:
    driver:bridge

Example:-
services:
  proxy:
    image: ngnix
    networks:
      - frontend

  app:
    image: myapp
    networks:
      - frontend
      - backend
  
  db:
    image: postgress
    networks: 
      - backend

network
  frontend:
    driver:bridge
  backend:
    driver:bridge
	
Dependence:- We can use depend on to use for running conatiner before the main one and used to set order for creating services in 
             the yaml file.

services:
  app:
    image: myapp
    depends_on:
      - db
  db:
    image: postgres 
    networks:
      - back-tier

Example:
services:
  web:
    image: ngnix:alpine
    ports:
      - '8080:80'
	networks:
	  - my_network
	depends_on:
	  - db 

  db:
    image: postgress
    ports:
      - '5432'
	networks:
	  - my_network  
	  
netoworks:
  my_network:
    driver:bridge

	
Docker Multi compose file for environment: We can copy the same and can use for other env like dev,prod and test.
                                           
Command: docker compose -f full filename.yml up -d 	
         docker compose -f full filename.yml down -- volumes 
         
		 To login in the mysql server use 
		 docker compose exec db mysql -u root -p
		 enter password:
		 show databases

Creating custom image: Use docker file and define the config in the file and later can be used in the compose file like this.

Example: dockerfile
FROM ubuntu
WORKDIR /app
COPY script.sh
RUN chomd 777 script.sh
CMD ["./script.sh"]	

services:
  app: 
    build: . (It will use the current docker file)
	  dockerfile: Dockerfile
  web:
    image: ngnix:alpine
    ports:
      - '8080:80'
	networks:
	  - my_network
	depends_on:
	  - db 

  db:
    image: postgress
    ports:
      - '5432'
	networks:
	  - my_network 
  

Example: dockerfile
FROM ubuntu
WORKDIR /app
COPY script.sh
RUN chomd 777 script.sh
CMD ["./script.sh"]		  
	

Volumes- Named:-

services:
  app:
    image: myapp
    depends_on:
      - db
   
  db:
    image: postgres
    volumes:
      - db-data:/etc/data ---->mapping / Append with optional flags :ro/:rw
    networks:
      - back-tier

volumes: ----> Volumes definition
  db-data:


Restart Policy:- * No
                     * The default restart policy.
                     *Does not restart a container under any circumstances.

                 * always
                     *Always restarts the container until it's removal

                 * on-failure
                     *Restarts a container if the exit code indicates an error.
                 * unless-stopped
                     *Restarts a container irrespective of the exit code but will stop
                      restarting when the service is stooped or removed.
           
Example-
services:
  app:
    image: myapp
    restart: always
    depends_on:
      - db

  db:
    image: postgress
    restart: always

Container registries:- *Central repo for containers images
                       *Private or/ public
                       *Docker Hub
                          *hub.docker.com

                       *Microsoft
                          *azure Container Registry
                          *Microsoft Container Registry (Public Images)
                            *mcr.micrososft.com


                       *Amazon Elastic Container Registry
                       *Google Container Registry


# tag the image previously built
docker tag my_image k8sacademy/my_image:latest

docker build -t <your_registry_name/expresss:v1>

#push the image with you re
docker push piykesh/myimage:latest
docker pull piykesh/myimage:latest


AWS ECS: Mangeed container orchestrator

*Manage lifecycles of container create/restart/destroy
*Deploy & load-balance appliction across multiple servers.
*Autpscaling to handle variance in traffic
*Rolling out changes to application.

Docker compose deployment:

*Docker compose was limited to on server
*We need more server we cannot rely to one server so we need to scale as we grow we need more servers and for that orchestrator
  is needed.

AWS ECS as orchestrator for containerization: 

*It has to launch types: EC2 and Fargate

ECS Cluster: It undelying infrastructure where we have bunch of virtual machines where ECS intance can deploy your application 
             on it.

EC2- We have to manage the undelying EC2 Instances.
   
              
			   | ECS Control Plane |
                        |
                        |
       --------------Cluster------------  
      |     EC2                  Ec2    |
      |  |      |             |       | |
	  |  | ECS  |             |  ECS  | |
	  |	 |Agent	|			  |	Agent |	|
	  |  |Docker|             | Docker| |
	  |  Firewall              Firewall |
 	  |   Patch                 Patch   |
	  
*You still need to manage the undelying infrastructure (EC2)
*ECS Manages the conatiner
*Full control over the infrastructure.


Fargate: AWS Manages the the undelying infrastructure.

      | ECS Control Plane        Fargate|
                        
                        |
       --------------Cluster------------  
      |                                 |
      |  Server             Server      |
	  |                                 |
	  |	 								|
	  |   								|
	  |                				    |
 	  |  							    |
	  
*Follows a serverless architecture
*Fargate will create server on demand.
*No need to provision/ Maintain EC2 servers
*You only need to pay for what you use.


EC2 Task Definition:
*Blueprint that describes how conatiner should launch.
*How much CPU/Mem
*Image/ports/Volumes.
*It basically conatins all the configuration we put in the docker compose file.
*A task is an instance of task definition is a instructions.
*A running containers with setting defined in the Task def.
*Whenever we go to create the tasl def we need the role to permit it (ecsTaskExecutionRole)
*Then define Operating System, Task Executionrole, Task Size, Container- (container name, image, Memory, ports, etc) Volumes, env
 
ECS Service:
*A service ensures that a certain number of Tasks are running at all times.
*Restart conatiner that have exited/crashed.
*Ec2 instance fails, the service will b restart on a working EC2 instance.

Load Balancer:
*A load balancer can be assigned to route external traffic to your service